{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recycling Analysis — Python\n",
    "\n",
    "Estimates gamma(t) from bulk LF data, then applies recycling correction to single-cell mTRAQ ratios.\n",
    "Mirrors `R/Recycling_analysis.R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.optimize import minimize_scalar, curve_fit\n",
    "from joblib import Parallel, delayed\n",
    "%matplotlib inline\n",
    "\n",
    "import quantqc as qqc\n",
    "from quantqc.normalize import normalize_reference_vector\n",
    "from quantqc.miceotope import miceotope_protein_collapse\n",
    "\n",
    "# Paths\n",
    "path_raw  = '/Users/andrewleduc/Desktop/jmod_raw/'\n",
    "data_out  = os.path.normpath(os.path.join(os.getcwd(), '../data_out_temp')) + '/'\n",
    "lf_path   = '/Users/andrewleduc/Desktop/Github/Kevin_PTI/report.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tls(vect1, vect2):\n",
    "    \"\"\"Total Least Squares slope via SVD. Matches R TLS().\"\"\"\n",
    "    vect1 = np.array(vect1, dtype=float)\n",
    "    vect2 = np.array(vect2, dtype=float)\n",
    "    vect1[~np.isfinite(vect1)] = np.nan\n",
    "    vect2[~np.isfinite(vect2)] = np.nan\n",
    "\n",
    "    int_x = np.nanmean(vect1)\n",
    "    int_y = np.nanmean(vect2)\n",
    "    v1 = vect1 - int_x\n",
    "    v2 = vect2 - int_y\n",
    "\n",
    "    mat = np.column_stack([v1, v2])\n",
    "    mat = mat[~np.isnan(mat).any(axis=1)]\n",
    "\n",
    "    _, _, Vt = np.linalg.svd(mat)\n",
    "    V = Vt.T\n",
    "    slope = V[0, 0] / V[1, 0]\n",
    "    return slope, (int_x, int_y)\n",
    "\n",
    "\n",
    "def L_function(alpha, L0, a, b, t):\n",
    "    \"\"\"Analytic solution for labelled fraction after recycling correction. Matches R L_function().\"\"\"\n",
    "    c = 0.5\n",
    "    beta = L0 * alpha\n",
    "    num = (\n",
    "        a * beta * c\n",
    "        - 2 * alpha * beta * c\n",
    "        + b * beta * c\n",
    "        + alpha * beta * c * np.exp((-a + alpha) * t)\n",
    "        - b * beta * c * np.exp((-a + alpha) * t)\n",
    "        - a * beta * c * np.exp((alpha - b) * t)\n",
    "        + alpha * beta * c * np.exp((alpha - b) * t)\n",
    "        - a * alpha * L0\n",
    "        + alpha ** 2 * L0\n",
    "        + a * b * L0\n",
    "        - alpha * b * L0\n",
    "    )\n",
    "    denom = (-a + alpha) * (alpha - b) * np.exp(alpha * t)\n",
    "    return num / denom\n",
    "\n",
    "\n",
    "def objective_function(alpha, L_measured, L0, a, b, t):\n",
    "    \"\"\"RSS between predicted and measured L for a single peptide × cell.\"\"\"\n",
    "    L_predicted = L_function(alpha, L0, a, b, t)\n",
    "    return (L_predicted - L_measured) ** 2\n",
    "\n",
    "\n",
    "def _optimise_column(j, L_mat, L0_mat, a, b, t):\n",
    "    \"\"\"Optimise alpha for all peptides in column j (one cell). Used by joblib.\"\"\"\n",
    "    n_rows = L_mat.shape[0]\n",
    "    col_res = np.full(n_rows, np.nan)\n",
    "    for i in range(n_rows):\n",
    "        L_meas = L_mat[i, j]\n",
    "        L0_val = L0_mat[i, j]\n",
    "        if np.isfinite(L_meas) and np.isfinite(L0_val):\n",
    "            try:\n",
    "                res = minimize_scalar(\n",
    "                    objective_function,\n",
    "                    bounds=(0, 10),\n",
    "                    method='bounded',\n",
    "                    args=(L_meas, L0_val, a, b, t),\n",
    "                )\n",
    "                col_res[i] = res.x\n",
    "            except Exception:\n",
    "                pass\n",
    "    return col_res\n",
    "\n",
    "\n",
    "def recycle_adjust_par(L_mat, L0_mat, t, a, b, ncores=4):\n",
    "    \"\"\"Parallel recycling correction. Returns alpha matrix same shape as L_mat.\n",
    "\n",
    "    L_mat  : (n_peptides x n_cells) measured L / (L+H) ratio\n",
    "    L0_mat : (n_peptides x n_cells) total intensity L+H\n",
    "    t      : labeling time (days)\n",
    "    a, b   : gamma(t) parameters from bulk LF data\n",
    "    \"\"\"\n",
    "    n_cols = L_mat.shape[1]\n",
    "    results = Parallel(n_jobs=ncores, verbose=1)(\n",
    "        delayed(_optimise_column)(j, L_mat, L0_mat, a, b, t)\n",
    "        for j in range(n_cols)\n",
    "    )\n",
    "    out = np.column_stack(results)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Estimate gamma(t) parameters from bulk LF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_data = pl.read_parquet(lf_path).to_pandas()\n",
    "\n",
    "# Rename runs to short labels\n",
    "LF_data.loc[LF_data['Run'] == '2026-01-14_2ngL2_20Th-10ms_28min_nce30', 'Run'] = 'L2'\n",
    "LF_data.loc[LF_data['Run'] == '2026-01-14_2ngL1_20Th-10ms_28min_nce30', 'Run'] = 'L1'\n",
    "\n",
    "LF_data = LF_data[LF_data['Ms1.Area'] != 0].copy()\n",
    "LF_data['seqcharge'] = LF_data['Stripped.Sequence'].astype(str) + LF_data['Precursor.Charge'].astype(str)\n",
    "\n",
    "# Keep only peptides with exactly 2 lysines\n",
    "LF_data['kcount'] = LF_data['Stripped.Sequence'].str.count('K')\n",
    "LF_data = LF_data[LF_data['kcount'] == 2]\n",
    "\n",
    "# Count heavy-labeled K residues\n",
    "LF_data['hcount'] = LF_data['Precursor.Id'].str.count('K_6C13-6')\n",
    "\n",
    "HL_dat = LF_data[LF_data['hcount'] == 1].copy()\n",
    "HH_dat = LF_data[LF_data['hcount'] == 2].copy()\n",
    "\n",
    "sect = set(HL_dat['seqcharge']) & set(HH_dat['seqcharge'])\n",
    "HL_dat = HL_dat[HL_dat['seqcharge'].isin(sect)]\n",
    "HH_dat = HH_dat[HH_dat['seqcharge'].isin(sect)]\n",
    "\n",
    "HL_piv = HL_dat.groupby(['seqcharge', 'Run'])['Precursor.Quantity'].median().reset_index()\n",
    "HL_piv = HL_piv.pivot(index='seqcharge', columns='Run', values='Precursor.Quantity')\n",
    "HH_piv = HH_dat.groupby(['seqcharge', 'Run'])['Precursor.Quantity'].median().reset_index()\n",
    "HH_piv = HH_piv.pivot(index='seqcharge', columns='Run', values='Precursor.Quantity')\n",
    "\n",
    "common_runs = HL_piv.columns.intersection(HH_piv.columns)\n",
    "HL_mat = HL_piv[common_runs].values\n",
    "HH_mat = HH_piv[common_runs].values\n",
    "\n",
    "pct_Heavy = 2 / (HL_mat / HH_mat + 2)\n",
    "\n",
    "# Complete cases only\n",
    "ok = ~np.isnan(pct_Heavy).any(axis=1)\n",
    "pct_Heavy_cc = pct_Heavy[ok]\n",
    "\n",
    "print(f'Complete-case peptides: {ok.sum()}')\n",
    "print(f'Runs: {list(common_runs)}')\n",
    "print(f'Median pct_Heavy by run:')\n",
    "for i, r in enumerate(common_runs):\n",
    "    print(f'  {r}: {np.nanmedian(pct_Heavy_cc[:, i]):.4f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(common_runs), figsize=(5 * len(common_runs), 4))\n",
    "if len(common_runs) == 1:\n",
    "    axes = [axes]\n",
    "for i, r in enumerate(common_runs):\n",
    "    axes[i].boxplot(pct_Heavy_cc[:, i])\n",
    "    axes[i].set_title(f'% Heavy — {r}')\n",
    "    axes[i].set_ylabel('pct_Heavy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit gamma(t) = 1 - 0.5*exp(-a*t) - 0.5*exp(-b*t)\n",
    "# Observed: t=0 -> 0, t=3 -> HH_fract_3day, t=5 -> HH_fract_5day\n",
    "# Update these from the medians above\n",
    "HH_fract_3day = 0.5    # update if run L1 = 3 day\n",
    "HH_fract_5day = 0.537  # update if run L2 = 5 day\n",
    "\n",
    "time_obs = np.array([0.0, 3.0, 5.0])\n",
    "recycle_obs = np.array([0.0, HH_fract_3day, HH_fract_5day])\n",
    "\n",
    "def gamma_model(t, a, b):\n",
    "    return 1 - 0.5 * np.exp(-a * t) - 0.5 * np.exp(-b * t)\n",
    "\n",
    "a_init = np.log(2) / 0.7\n",
    "b_init = np.log(2) / 20\n",
    "\n",
    "popt, _ = curve_fit(gamma_model, time_obs, recycle_obs, p0=[a_init, b_init],\n",
    "                    bounds=([0, 0], [np.inf, np.inf]), maxfev=10000)\n",
    "a_fitted, b_fitted = popt\n",
    "print(f'a_fitted = {a_fitted:.4f}, b_fitted = {b_fitted:.4f}')\n",
    "\n",
    "t_plot = np.linspace(0, 20, 200)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(t_plot, gamma_model(t_plot, a_fitted, b_fitted), 'b-', label='Fitted gamma(t)')\n",
    "ax.scatter(time_obs, recycle_obs, color='red', zorder=5, label='Observed')\n",
    "ax.set_xlabel('Time (days)')\n",
    "ax.set_ylabel('Fraction heavy AA')\n",
    "ax.set_title('Fitted gamma(t): free AA labeling kinetics')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load preprocessed QQC object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_out, 'r1_5day_male.pkl'), 'rb') as f:\n",
    "    r1_5day_male = pickle.load(f)\n",
    "\n",
    "meta = r1_5day_male.meta_data\n",
    "Raw_L = r1_5day_male.miceotopes.Raw_L\n",
    "Raw_H = r1_5day_male.miceotopes.Raw_H\n",
    "miceotope_cols = r1_5day_male.miceotopes.cols\n",
    "\n",
    "print(f'Raw_L shape: {Raw_L.shape}')\n",
    "print(f'Meta shape:  {meta.shape}')\n",
    "print(f'Samples:     {meta[\"sample\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split cells by mouse (3-day vs 5-day labeling)\n",
    "days_3_ids = meta.loc[meta['sample'] == 'mouse1', 'ID'].values\n",
    "days_5_ids = meta.loc[meta['sample'] == 'mouse2', 'ID'].values\n",
    "\n",
    "# Keep only IDs present in the miceotope matrix\n",
    "days_3_ids = np.array([x for x in days_3_ids if x in miceotope_cols])\n",
    "days_5_ids = np.array([x for x in days_5_ids if x in miceotope_cols])\n",
    "\n",
    "col_idx  = {c: i for i, c in enumerate(miceotope_cols)}\n",
    "idx3 = np.array([col_idx[x] for x in days_3_ids])\n",
    "idx5 = np.array([col_idx[x] for x in days_5_ids])\n",
    "\n",
    "print(f'3-day cells: {len(idx3)}, 5-day cells: {len(idx5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Compute raw L/(L+H) ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L3 = Raw_L[:, idx3].astype(float)\n",
    "H3 = Raw_H[:, idx3].astype(float)\n",
    "L5 = Raw_L[:, idx5].astype(float)\n",
    "H5 = Raw_H[:, idx5].astype(float)\n",
    "\n",
    "r1 = L3 / (L3 + H3)   # L/(L+H) for 3-day mouse\n",
    "r2 = L5 / (L5 + H5)   # L/(L+H) for 5-day mouse\n",
    "\n",
    "min_ratio_3 = 1 - HH_fract_3day\n",
    "min_ratio_5 = 1 - HH_fract_5day\n",
    "\n",
    "pct_below_3 = np.sum(r1 < min_ratio_3, where=np.isfinite(r1), initial=0) / np.sum(np.isfinite(r1))\n",
    "pct_below_5 = np.sum(r2 < min_ratio_5, where=np.isfinite(r2), initial=0) / np.sum(np.isfinite(r2))\n",
    "print(f'Fraction below min ratio (3-day): {pct_below_3:.3f} (~10% expected)')\n",
    "print(f'Fraction below min ratio (5-day): {pct_below_5:.3f} (~20% expected)')\n",
    "\n",
    "# NA out physically impossible ratios\n",
    "r1[r1 < min_ratio_3] = np.nan\n",
    "r2[r2 < min_ratio_5] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Apply recycling correction (parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L0_mat = total intensity (L + H)\n",
    "tot3 = L3 + H3\n",
    "tot5 = L5 + H5\n",
    "\n",
    "print('Running recycling correction for 3-day mouse...')\n",
    "adj_r1 = recycle_adjust_par(r1, tot3, t=3, a=a_fitted, b=b_fitted, ncores=4)\n",
    "\n",
    "print('Running recycling correction for 5-day mouse...')\n",
    "adj_r2 = recycle_adjust_par(r2, tot5, t=5, a=a_fitted, b=b_fitted, ncores=4)\n",
    "\n",
    "# NA out boundary values (optimizer hit bounds → unreliable)\n",
    "adj_r1[adj_r1 >= 10] = np.nan\n",
    "adj_r1[adj_r1 <= 0]  = np.nan\n",
    "adj_r2[adj_r2 >= 10] = np.nan\n",
    "adj_r2[adj_r2 <= 0]  = np.nan\n",
    "\n",
    "print(f'adj_r1 finite: {np.sum(np.isfinite(adj_r1))}')\n",
    "print(f'adj_r2 finite: {np.sum(np.isfinite(adj_r2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrected degradation rates: -log(L/(L+H)) / t\n",
    "r1_rate = -np.log(r1) / 3\n",
    "r2_rate = -np.log(r2) / 5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(r1_rate.flatten(), adj_r1.flatten(), s=1, alpha=0.1, rasterized=True)\n",
    "ax.axline((0, 0), slope=1, color='red', linestyle='--')\n",
    "ax.set_xlim(0, 2.5)\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.set_xlabel('Uncorrected rate (day$^{-1}$)')\n",
    "ax.set_ylabel('Recycling-corrected rate (day$^{-1}$)')\n",
    "ax.set_title('Effect of recycling correction (3-day)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Compare 3-day vs 5-day corrected rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to peptides with >= 20 observations in both groups\n",
    "n_obs_r1 = np.sum(np.isfinite(adj_r1), axis=1)\n",
    "n_obs_r2 = np.sum(np.isfinite(adj_r2), axis=1)\n",
    "keep = (n_obs_r1 >= 20) & (n_obs_r2 >= 20)\n",
    "print(f'Peptides with >=20 obs in both groups: {keep.sum()}')\n",
    "\n",
    "adj_r1_ = adj_r1[keep]\n",
    "adj_r2_ = adj_r2[keep]\n",
    "\n",
    "mean_r1 = np.nanmean(np.log2(adj_r1_), axis=1)\n",
    "mean_r2 = np.nanmean(np.log2(adj_r2_), axis=1)\n",
    "\n",
    "ok = np.isfinite(mean_r1) & np.isfinite(mean_r2)\n",
    "cor_corrected = np.corrcoef(mean_r1[ok], mean_r2[ok])[0, 1]\n",
    "slope, _ = tls(mean_r1[ok], mean_r2[ok])\n",
    "print(f'Corrected correlation: {cor_corrected:.4f}')\n",
    "print(f'TLS slope (log2 rate 3d vs 5d): {slope:.4f}  (1/slope = {1/slope:.4f})')\n",
    "\n",
    "X = 2 ** mean_r1[ok]\n",
    "Y = 2 ** mean_r2[ok]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X, Y, s=8, alpha=0.5)\n",
    "ax.axline((0, 0), slope=1, linestyle='--', color='gray')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('3-day degradation rate (day$^{-1}$)')\n",
    "ax.set_ylabel('5-day degradation rate (day$^{-1}$)')\n",
    "ax.set_title(f'Recycling-corrected rates\\nr = {cor_corrected:.3f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Manual slope correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r1_log_tune = np.log2(adj_r1)\n",
    "adj_r2_log_tune = np.log2(adj_r2)\n",
    "\n",
    "# Compute offset: mean difference between 3-day and 5-day (in log2 space)\n",
    "mean_r1_all = np.nanmean(adj_r1_log_tune[keep], axis=1)\n",
    "mean_r2_all = np.nanmean(adj_r2_log_tune[keep], axis=1)\n",
    "ok_all = np.isfinite(mean_r1_all) & np.isfinite(mean_r2_all)\n",
    "\n",
    "tls_slope, _ = tls(mean_r1_all[ok_all], mean_r2_all[ok_all])\n",
    "offset = np.nanmean(mean_r1_all[ok_all] - mean_r2_all[ok_all])\n",
    "print(f'TLS slope: {tls_slope:.4f}, offset: {offset:.4f}')\n",
    "\n",
    "# Apply correction to 3-day (as in R)\n",
    "adj_r1_log_tune = adj_r1_log_tune * (1 / tls_slope) - offset\n",
    "\n",
    "# Re-check\n",
    "mean_r1_t = np.nanmean(adj_r1_log_tune[keep], axis=1)\n",
    "mean_r2_t = np.nanmean(adj_r2_log_tune[keep], axis=1)\n",
    "ok_t = np.isfinite(mean_r1_t) & np.isfinite(mean_r2_t)\n",
    "cor_tuned = np.corrcoef(mean_r1_t[ok_t], mean_r2_t[ok_t])[0, 1]\n",
    "print(f'Tuned correlation: {cor_tuned:.4f}')\n",
    "\n",
    "X_t = 2 ** mean_r1_t[ok_t]\n",
    "Y_t = 2 ** mean_r2_t[ok_t]\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X_t, Y_t, s=8, alpha=0.5)\n",
    "ax.axline((0, 0), slope=1, linestyle='--', color='gray')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0.01, 1)\n",
    "ax.set_ylim(0.01, 1)\n",
    "ax.set_xlabel('3-day rate (day$^{-1}$)')\n",
    "ax.set_ylabel('5-day rate (day$^{-1}$)')\n",
    "ax.set_title('After manual slope correction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Compare corrected vs uncorrected (reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrected rates for the same peptide set\n",
    "r1_rate_f = r1_rate[keep]\n",
    "r2_rate_f = r2_rate[keep]\n",
    "mean_unc_r1 = np.nanmean(np.log2(r1_rate_f), axis=1)\n",
    "mean_unc_r2 = np.nanmean(np.log2(r2_rate_f), axis=1)\n",
    "ok_unc = np.isfinite(mean_unc_r1) & np.isfinite(mean_unc_r2)\n",
    "cor_unc = np.corrcoef(mean_unc_r1[ok_unc], mean_unc_r2[ok_unc])[0, 1]\n",
    "tls_unc, _ = tls(mean_unc_r1[ok_unc], mean_unc_r2[ok_unc])\n",
    "print(f'Uncorrected correlation: {cor_unc:.4f}')\n",
    "print(f'Uncorrected TLS 1/slope: {1/tls_unc:.4f}')\n",
    "\n",
    "X_u = 2 ** mean_unc_r1[ok_unc]\n",
    "Y_u = 2 ** mean_unc_r2[ok_unc]\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X_u, Y_u, s=8, alpha=0.5)\n",
    "ax.axline((0, 0), slope=1, linestyle='--', color='gray')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('3-day rate (day$^{-1}$)')\n",
    "ax.set_ylabel('5-day rate (day$^{-1}$)')\n",
    "ax.set_title(f'Uncorrected rates\\nr = {cor_unc:.3f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Half-life distribution: corrected vs uncorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r1_tune = 2 ** adj_r1_log_tune\n",
    "adj_r2_tune = 2 ** adj_r2_log_tune\n",
    "\n",
    "hl_unc   = np.log(2) / np.nanmean(r2_rate_f, axis=1)\n",
    "hl_corr  = np.log(2) / np.nanmean(adj_r2_tune[keep], axis=1)\n",
    "\n",
    "ok_hl = np.isfinite(hl_unc) & np.isfinite(hl_corr) & (hl_unc > 0) & (hl_corr > 0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=False)\n",
    "for ax, vals, label in zip(axes, [hl_unc[ok_hl], hl_corr[ok_hl]], ['Uncorrected', 'Corrected']):\n",
    "    ax.hist(np.log10(vals), bins=30, color='steelblue' if label == 'Uncorrected' else 'black',\n",
    "            alpha=0.7, edgecolor='white')\n",
    "    ax.set_xlabel('log10 half-life (days)')\n",
    "    ax.set_ylabel('# proteins')\n",
    "    ax.set_title(f'{label} clearance half-life (5-day)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Update QQC object and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine corrected alpha (Kr) matrices back into miceotope cols order\n",
    "adj_all = np.full((adj_r1_tune.shape[0], len(miceotope_cols)), np.nan)\n",
    "for k, col_id in enumerate(days_3_ids):\n",
    "    adj_all[:, col_idx[col_id]] = adj_r1_tune[:, k]\n",
    "for k, col_id in enumerate(days_5_ids):\n",
    "    adj_all[:, col_idx[col_id]] = adj_r2_tune[:, k]\n",
    "\n",
    "# Replace Alpha_pep with recycling-corrected values\n",
    "r1_5day_male.miceotopes.Alpha_pep = adj_all\n",
    "\n",
    "# Collapse to protein level\n",
    "r1_5day_male = miceotope_protein_collapse(r1_5day_male)\n",
    "\n",
    "print(f'Alpha_prot shape: {r1_5day_male.miceotopes.Alpha_prot.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated QQC\n",
    "with open(os.path.join(data_out, 'r1_5day_male.pkl'), 'wb') as f:\n",
    "    pickle.dump(r1_5day_male, f)\n",
    "print('Saved updated QQC pickle.')\n",
    "\n",
    "# Save clearance rate matrices\n",
    "Alpha_prot = r1_5day_male.miceotopes.Alpha_prot\n",
    "prot_rows  = r1_5day_male.miceotopes.rows\n",
    "prot_cols  = r1_5day_male.miceotopes.cols\n",
    "\n",
    "# Relative (row-normalized)\n",
    "alpha_norm = normalize_reference_vector(Alpha_prot.copy(), log=True)\n",
    "pd.DataFrame(alpha_norm, index=prot_rows, columns=prot_cols).to_csv(\n",
    "    os.path.join(data_out, 'clearance_relative.csv'))\n",
    "\n",
    "# Absolute\n",
    "pd.DataFrame(Alpha_prot, index=prot_rows, columns=prot_cols).to_csv(\n",
    "    os.path.join(data_out, 'clearance_absolute.csv'))\n",
    "\n",
    "print('Saved clearance_relative.csv and clearance_absolute.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
